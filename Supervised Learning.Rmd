---
title: "IS7036 Supervised Learning"
author: "Keya Satpathy"
output: html_document
---
## Predicting Bankruptcy of a Company {.tabset .tabset-fade .tabset-pills}

### Introduction {.tabset .tabset-fade}
<div style="text-align: justify">

<p>**Problem Statement:** To build the best model for predicting the bankruptcy of a company from several financial variables that contain information about the company’s assets, sales, etc.</p>

</div>

#### About Data

<div style="text-align: justify">

<p>The bankruptcy dataset can be found [here](https://github.com/madhavachandra/Machine-Learning-on-Bankruptcy/blob/master/bankruptcy.csv). This dataset contains the companies’ financial information and their bankruptcy status for particular years. It is a subset of Man Xu’s project containing the financial information of the companies. The dataset contains 5436 observations with 13 variables. There are 13 attributes in this dataset. They are given in the table below.</p>

</div>

<div>
  <table class="table table-bordered table-striped">
    <thead>
        <tr>
            <th align="justify">Variable</th>
            <th align="justify">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align="justify">FYEAR</td>
            <td align="justify">Fiscal Year for which the data has been taken.</td>
        </tr>
        <tr>
            <td align="justify">DLRSN</td>
            <td align="justify">It is a Bankruptcy/Non-Bankruptcy Flag. Bankruptcy = 1 and Non-Bankruptcy = 0.</td>
        </tr>
        <tr>
            <td align="justify">CUSIP</td>
            <td align="justify">It is also known as the Committee on Uniform Securities Identification Procedures number, is a unique nine-character identification number assigned to all stocks (and registered bonds) in the U.S. and Canada.</td>
        </tr>
        <tr>
            <td align="justify">R1</td>
            <td align="justify">Working Capital / Total Asset</td>
        </tr>
        <tr>
            <td align="justify">R2</td>
            <td align="justify">Retained Earning / Total Asset</td>
        </tr>
        <tr>
            <td align="justify">R3</td>
            <td align="justify">Earning Before Interest & Tax/Total Asset</td>
        </tr>
        <tr>
            <td align="justify">R4</td>
            <td align="justify">Market Capital / Total Liability</td>
        </tr>
        <tr>
            <td align="justify">R5</td>
            <td align="justify">Sale / Total Asset</td>
        </tr>
        <tr>
            <td align="justify">R6</td>
            <td align="justify">Total Liability / Total Asset</td>
        </tr>
        <tr>
            <td align="justify">R7</td>
            <td align="justify">Current Asset / Current Liability</td>
        </tr>
        <tr>
            <td align="justify">R8</td>
            <td align="justify">Net Income / Total Asset</td>
        </tr>
        <tr>
            <td align="justify">R9</td>
            <td align="justify">It is Sales data. But as the sales number was huge, so log has been taken on sales.</td>
        </tr>
        <tr>
            <td align="justify">R10</td>
            <td align="justify">The market capital of a company in a particular Fiscal Year. This data exist on the log-scale.</td>
        </tr>
    </tbody>
  </table>
</div>

#### Import and Explore Data

<div style="text-align: justify">

**Required Packages:**

```{r, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r setup, message=FALSE, warning=FALSE}
library(dplyr)
library(rpart)
library(rpart.plot)
library(GGally)
library(fields)
library(randomForest)
library(verification)
library(nnet)
library(neuralnet)
library(tidyr)
library(knitr)
library(ggplot2)
library(reshape2)
```

**Import Data using `read.csv()`:**

```{r}
bankruptcy <- read.csv("E:/MSIS/Spring 2020/Flex 2/IS 7036/bankruptcy.csv", h=T, stringsAsFactors = FALSE)
```

<p>As, CUSIP is an assigned ID and Fiscal Year won’t play much role in this subset of Man Xu’s dataset, therefore, we will drop these columns. In this analysis, we will use various supervised learning techniques to build models with the dependent variable – DLRN and R1-R10 as the predictor variables. </p>
 
```{r}
bankruptcy <- bankruptcy[-c(1,3)]
str(bankruptcy)
head(bankruptcy)
colnames(bankruptcy)
```
</div>

#### Data Sampling

<div style="text-align: justify">

<p>We have split the original data in 70:30 ratio to create training and testing data.</p>

```{r}
set.seed(13232767)
sample_index <- sample(nrow(bankruptcy),nrow(bankruptcy)*0.70)
bankruptcy.train <- bankruptcy[sample_index,]
bankruptcy.test <- bankruptcy[-sample_index,]
```

<p>We have used `set.seed()` function to ensure we get same samples for test and train such that the results don't vary.</p>
</div>

### Exploratory Data Analysis

<div style="text-align: justify">

<p>We begin with summary statistics of all the variables of the dataset.</p>

```{r}
summary(bankruptcy)
```


<p>From summary analysis, we see that none of the variables are having factors. Also, with domain understanding, it is clear that no variables should be converted into factors. DLRSN is the only variable which is 0/1 and this variable will be used as a predictor for logistic regression.</p>

<p>R10 which is the market cap and R9 which is the sales data is in log-scale. </p>

<p>We have also checked for null values. Checking the null values in dataset is very important, and if we find any, then we have to impute the values, either with mean or median or depending upon the domain. There are no Null Values in the dataset.</p>


<p>Next, we check the distributions of all independent variables.</p>

```{r}
par(mfrow = c(3,4))
par(mfrow = c(3,4))
i <- 4
for (i in 2:11) 
{
  hist((bankruptcy[,i]), main = paste("Distibution of ", colnames(bankruptcy[i])), xlab = colnames(bankruptcy[i]))
}
```

<p>R9 and R10 seem to be close to a normal distribution and nothing concrete can be commented about the others. R2 and R4 have heavy tails. </p>

<p>Let us see what is the distribution of the bankruptcy indicator in the dataset.</p>

```{r}
par(mfrow = c(1,1))
barplot(table(bankruptcy$DLRSN), main = "Frequency of Bankruptcy Flag")
```

<p>Number of Non- Bankrupt Companies are way more than the Bankrupt ones in the data set.</p>

<p>Next, we check for outliers using boxplots.</p>

```{r}
d <- melt(bankruptcy)
ggplot(d,aes(x = variable, y= value)) +
  facet_wrap(~variable,scales = "free_x") +
  geom_boxplot()
```

<p>From the plots, it is clear that R1, R4, R5, R6 and R7 have outliers where as R2,R3, R8, R9, and R10 have no outliers.</p>

<p>Now let us check the correlation between independent variables and the bankruptcy flag.</p>

```{r}
ggcorr(bankruptcy, label = TRUE)
```

<p>We can see that R10 has the highest negative correlation with the bankruptcy flag with the correlation coefficient as -0.4 followed by R6 and R3 with a correlation coefficient of 0.3 and -0.3 respectively. We see a high correlation between R8 and R3 and moderate correlation among other variables.</p>

</div>

### Supervised Learning Methods {.tabset .tabset-fade}

<div style="text-align: justify">

<p>The supervised learning methods that we have used are:</p>

<ul>
  <li>Generalized Logistic Regression</li>
  <li>Classification Tree</li>
  <li>Random Forest</li>
  <li>Artificial Neural Network</li>
</ul>

</div>

#### Generalized Logistic Regression

<div style="text-align: justify">
<p>There are two models in the logistic regression: </p>
<ul>
  <li>using logit link</li>
  <li>using probit link</li>
</ul>

<p>The logit model uses the cumulative distribution function of the logistic distribution whereas the probit model uses the cumulative distribution function of the standard normal distribution.</p>

**Logistic Regression Using Logit Link:**

```{r}
bankruptcy.glm.logit <- glm(DLRSN ~ ., family = binomial(link="logit"), data=bankruptcy.train)
summary(bankruptcy.glm.logit)
bankruptcy.glm.logit$deviance
AIC(bankruptcy.glm.logit)
BIC(bankruptcy.glm.logit)
```


**Model selection using the BIC criterion:**

<p>We build two models. First model is the null model containing only the bankruptcy indicator and no predictors. The second model is the full model containing all predictors.</p>

<p>Now we select the best model using the stepwise selection method - with BIC criterion.</p>

```{r}
model.full <- glm(DLRSN ~ R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + R9 + R10, family = "binomial" ,data = bankruptcy.train)
model.null <- glm(DLRSN ~ 1, family = "binomial" ,data = bankruptcy.train)
step.model <- step(model.null, scope = list(lower = model.null, upper = model.full), direction = "both", k = log(nrow(bankruptcy.train)))
summary(step.model)
AIC(step.model)
BIC(step.model)
```

<p>The model with BIC criterion has 8 predictors namely R1, R2, R3, R4, R6, R7, R8 and R10.</p>


**In-Sample performance of the model:**

<p>Once we have the final logistic regression model, we will plot the ROC curve to get the area under the curve. The ROC curve is plot of Sensitivity (True Positives) vs Specificity (False Positives). Higher area under the curve, better is the model performance.</p>

```{r}
final.model <- glm(DLRSN ~ R1 + R2 + R3 + R4+ R6 + R7 + R8 + R10, family = "binomial" ,data = bankruptcy.train)

prob.insample <- predict(final.model, type = "response")
roc.plot(bankruptcy.train$DLRSN == "1", prob.insample)$roc.vol

```

<p>The above figure shows the ROC Curve for the training Dataset and the Area under the Curve (AUC) obtained is 88.3%.</p>


**Cost Function:**

<p>For obtaining the misclassification rate, we need to decide the threshold for categorizing the bankruptcy status. By using the cost function, we will plot a graph of Cost of Training data vs threshold value and choose the value with lowest cost as the threshold for classifying the bankruptcy status. The ratio of weightage for False Positive and False Negative is 1:35. Here we are trying to say that wrongly predicting the companies which will go bankrupt as Non-Bankrupt is more serious and risky than predicting the vis versa. Therefore, it is important to reduce the False Negatives.</p>

**Optimal Cutoff:**

```{r}
searchgrid = seq(0.01, 0.99, 0.01)
result = cbind(searchgrid, NA)

cost1 <- function(r, pi) {
  weight1 = 35
  weight0 = 1
  c1 = (r == 1) & (pi < cutoff)  #logical vector - true if actual 1 but predict 0
  c0 = (r == 0) & (pi > cutoff)  #logical vecotr - true if actual 0 but predict 1
  return(mean(weight1 * c1 + weight0 * c0))
}

for (i in 1:length(searchgrid)) {
  cutoff <- result[i, 1]
  # assign the cost to the 2nd col
  result[i, 2] <- cost1(bankruptcy.train$DLRSN, predict(final.model, type = "response"))
}
plot(result, ylab = "Cost in Training Set", main = "Asymmetric Cost Function")

opt.cutoff <- searchgrid[which(result[,2]==min(result[,2]))]
opt.cutoff

```

<p>The optimal cutoff probability comes out to be 0.03.</p>


**In-Sample performance of the model:**

<p>We now calculate the misclassification rate of the training dataset using this threshold value.</p>

```{r}
bankruptcy.train$pred <-  ifelse(prob.insample>opt.cutoff,1,0)
table(pred = bankruptcy.train$pred , true = bankruptcy.train$DLRSN)
mean(bankruptcy.train$pred != bankruptcy.train$DLRSN)
```

<p>The misclassification rate obtained for training dataset is 0.4751643.This means our model is around 53% accurate in predicting the bankruptcy status.</p>


**Out-of-Sample Performance of the Model:**

<p>Now, let us test the performance of the model on the test dataset. The 30% of the data sampled initially will be used as the test dataset. To gauge the model performance on the test data, we will predict the bankruptcy status for the companies present in the test dataset using the Final Logistic Regression Model built above. We will check for the asymmetric misclassification rate and the Area under the Curve for test dataset.</p>

<p>Let us plot the ROC Curve for test dataset.</p>

```{r}
prob.outsample <- predict(final.model, bankruptcy.test, type = "response")
roc.plot(bankruptcy.test$DLRSN == "1", prob.outsample)$roc.vol
```

<p>The Area Under the Curve (AUC) for the test dataset is 87%.</p>

<p>We now calculate the asymmetric classification rate for the test data set. It is to be noted that we will be using the value of cut-off probability as 0.03(the value we had derived using the cost function).</p>

```{r}
bankruptcy.test$pred <-  ifelse(prob.outsample>opt.cutoff,1,0)
table(pred = bankruptcy.test$pred , true = bankruptcy.test$DLRSN)

mean(bankruptcy.test$pred != bankruptcy.test$DLRSN)
```

<p>The misclassification rate obtained for the test dataset is 0.4825261. This means our model shows nearly 52% accuracy in predicting the bankruptcy status for observations in the test dataset.</p>

<p>The misclassification rate and AUC (Area Under Curve) is close to each other for the training as well as the test dataset. The AUC for training dataset is slightly higher than the test data while the misclassification rate for the test data is slightly higher than the training dataset.</p>

<p>From the above process, we can say that the Logistic Regression model built has a decent performance for training as well as test dataset and is able to predict the bankruptcy status of the companies with an accuracy of around 52%.Therefore, for out-of-sample model is performing good considering our domain here. It is better to predict a company as bankrupt initially rather than loose out on the principal amount after the company actually goes bankrupt.</p>

**Logistic Regression Using Probit Link:**

We did this just for comparision with logit link.

```{r}
bankruptcy.glm.probit<- glm(DLRSN~., family=binomial(link="probit"), data=bankruptcy.train)
summary(bankruptcy.glm.probit)
bankruptcy.glm.probit$deviance
AIC(bankruptcy.glm.probit)
BIC(bankruptcy.glm.probit)
```

<p>Based on the above summary statistics for Logit and Probit links, we can observe that ‘R4’, ‘R5’ and ‘R9’ seems to be statistically insignificant as they have a p-value greater than 0.05, whereas other covariates are significant as they have a p-value less than 0.05.</p>

</div>

#### Classification Tree

<div style="text-align: justify">
<p>We have chosen the default complexity parameter (cp) value of 0.001 and 35:1 asymmetric cost to build the classification tree as below.</p>

```{r}
bankruptcy.rpart <- rpart(formula = DLRSN ~ ., data = bankruptcy.train, method = "class", 
                          parms = list(loss=matrix(c(0,35,1,0), nrow = 2)))
prp(bankruptcy.rpart, extra = 1)

bankruptcy.train.pred.tree <- predict(bankruptcy.rpart)
bankruptcy.test.pred.tree <- predict(bankruptcy.rpart,bankruptcy.test)

MSE.tree <- mean((bankruptcy.train.pred.tree-bankruptcy.train$DLRSN)^2)
MSPE.tree <- mean((bankruptcy.test.pred.tree-bankruptcy.test$DLRSN)^2)
```

<p>Checking for the MSPE and test MSE value, we get a test error of 0.3988483 and training error of 0.4001128. Then by using the ‘plotcp’ function, we have determined the best cp value in order to minimize the prediction error. The cp is used to control the size of the decision tree and to select the optimal tree size. If the cost of adding another variable to the decision tree from the current node is above the value of cp, then tree building does not continue. The best cp value for our model is 0.011 as obtained from the elbow graph shown below.</p>

<p>Here we are applying pruning to remove the sections of the classification tree that provide little power to classify instances. It reduces the complexity of the final tree and hence improves the overall predictive accuracy by reducing chances of overfitting.</p>

**Pruning with cp:**

```{r}
plotcp(bankruptcy.rpart)
```

<p>The pruned tree with cp = 0.011 is as below.</p>

**MSE for pruned tree:**

<p>We create a pruned tree using complexity parameter obtained from plotcp.</p>

```{r}
bankruptcy.pruned.tree <- prune(bankruptcy.rpart, cp = 0.011)
prp(bankruptcy.pruned.tree,digits = 4, extra = 1)
mean((predict(bankruptcy.pruned.tree) - bankruptcy.train$DLRSN)^2)
```


**MSPE for pruned tree:**

```{r}
bankruptcy.test.pred.pruned.tree = predict(bankruptcy.pruned.tree,bankruptcy.test)
mean((bankruptcy.test.pred.pruned.tree - bankruptcy.test$DLRSN)^2)
```

<p>Checking for the MSPE and test MSE value with the best cp value, we get a test error of 0.3987942 and training error of 0.3999889.</p>

</div>

#### Random Forest

<div style="text-align: justify">
<p>The idea of random forests is to randomly select m out of p predictors as candidate variables for each split in each tree. The reason of doing this is that it can decorrelates the trees such that it reduces variance when we aggregate the trees.</p>

```{r}
bankruptcy.rf<- randomForest(as.factor(DLRSN)~., data = bankruptcy.train)
bankruptcy.rf
```

<p>The out-of-bag prediction is similar to LOOCV. We use full sample. In every bootstrap, the unused sample serves as testing sample, and testing error is calculated. In the end, OOB error, root mean squared error by default, is obtained. In this case, our OOB estimate of error rate is 11.25%.</p>

**Variable Importance:** The top 2 most important features are R10 and R4.

```{r}
varImpPlot(bankruptcy.rf)
```


**Plotting the error rate vs. ntree:**

```{r}
plot(bankruptcy.rf, lwd=rep(2, 3))
legend("right", legend = c("OOB Error", "FPR", "FNR"), lwd=rep(2, 3), lty = c(1,2,3), col = c("black", "red", "green"))
```

**Misclassification rate- MSE:**

```{r}
bankruptcy.train.pred.tree = predict(bankruptcy.rf)
mean(ifelse(bankruptcy.train$DLRSN != bankruptcy.train.pred.tree, 1, 0))
```

**Misclassification rate- MSPE:**

```{r}
bankruptcy.test.pred.tree = predict(bankruptcy.rf, bankruptcy.test)
mean(ifelse(bankruptcy.test$DLRSN != bankruptcy.test.pred.tree, 1, 0))
```

</div>

#### Artificial Neural Network

<div style="text-align: justify">

<p>We are building a Neural Network for a categorical response now. For classification problems with nnet, we need to code the response to factor first. In addition we will the add type = “class” for the function.</p>

<p>A neuron is the basic unit of a neural network. It gets certain number of inputs and a bias value. When a signal arrives, it gets multiplied by a weight value. A weight represent the strength of the connection between units. If the weight from node 1 to node 2 has greater magnitude, it means that neuron 1 has greater influence over neuron 2. Weight increases the steepness of activation function. This means weight decide how fast the activation function will trigger.</p>

<p>Bias value allows the shifting the activation function to the left or right, which is critical for successful learning. The output is calculated by multiplying the input by the weight and passing the result through the activation function.</p>

```{r}
bankruptcy.nnet <- nnet(DLRSN ~ ., data = bankruptcy.train, size = 1, maxit = 500, type='class')
```


**In-sample confusion matrix for NN:**

```{r}
prob.nnet.train = predict(bankruptcy.nnet, bankruptcy.train)
pred.nnet.train = as.numeric(prob.nnet.train > opt.cutoff)
table(bankruptcy.train$DLRSN, pred.nnet.train, dnn = c("Observed","Predicted"))

```

**Out-of-sample confusion matrix for NN:**

```{r}
prob.nnet.test = predict(bankruptcy.nnet, bankruptcy.test)
pred.nnet.test = as.numeric(prob.nnet.test > opt.cutoff)
table(bankruptcy.test$DLRSN, pred.nnet.test, dnn = c("Observation", "Prediction"))

```

**Misclassification Rate - MSE:**

```{r}
mean(ifelse(bankruptcy.train$DLRSN != pred.nnet.train, 1, 0))
```

**Misclassification Rate - MSPE:**

```{r}
mean(ifelse(bankruptcy.test$DLRSN != pred.nnet.test, 1, 0))

nn <- neuralnet(DLRSN ~ R1+R2+R3+R4+R5+R6+R7+R8+R9+R10,
                data=bankruptcy.train,hidden=c(8,1),linear.output=FALSE,stepmax = 1e6) 
plot(nn, rep = "best")

```
</div>


### Model Comparison and Conclusion

<div style="text-align: justify">
<p>Finally, we have compared all models based on MSE and MSPE.</p>

```{r, warning= FALSE}
model = factor(c("Logit", "Tree", "RF", "NN"),
               levels=c("Logit", "Tree", "RF", "NN"))

train_mse <- c(
  mean(bankruptcy.train$pred != bankruptcy.train$DLRSN),
  mean((predict(bankruptcy.pruned.tree) - bankruptcy.train$DLRSN)^2),
  mean(ifelse(bankruptcy.train$DLRSN != bankruptcy.train.pred.tree, 1, 0)),
  mean(ifelse(bankruptcy.train$DLRSN != pred.nnet.train, 1, 0))
  )

test_mspe <- c(
  mean(bankruptcy.test$pred != bankruptcy.test$DLRSN),
  mean((bankruptcy.test.pred.pruned.tree - bankruptcy.test$DLRSN)^2),
  mean(ifelse(bankruptcy.test$DLRSN != bankruptcy.test.pred.tree, 1, 0)),
  mean(ifelse(bankruptcy.test$DLRSN != pred.nnet.test, 1, 0))
)

comparison_table <- data.frame(model=model,
                               train = train_mse,
                               test = test_mspe)

 

comparison_table$train <- round(comparison_table$train,2)
comparison_table$test <- round(comparison_table$test,2)

comparison_table1 <- gather(comparison_table, subset, mspe, 2:3)

kable(comparison_table)

ggplot(comparison_table1, aes(x=model, y=mspe, group=subset, label=mspe)) +
  geom_line(linetype="dashed", size=1.2)+
  geom_point(size=3) +
  geom_label(show_guide  = F) 

```


<p>From the summary, we find that MSPE for logistic regression is 0.48, decision tree is 0.4, random forest is 0.11 and neural network is 0.14. It can be observed that while logistic regression and decision tree gives almost similar MSPE; random forest and neural network also gives approximately similar MSPE. Though all the MSPE values are good enough, we can go ahead with logistic regression or decision tree if model simplicity is the ultimate criterion. A logistic regression tends to be less susceptible to overfitting when compared to decision trees. Another thing to consider is that decision trees automatically consider interactions between variables while we must manually add those interaction terms in logistic regression.</p>

<p>A random forest gives a better interpretation compared to decision trees as it decorrelates the trees, thereby reducing variance when we aggregate the trees.  A neural network is a black box with greater computational burden and proneness to overfitting. </p>

<p>In conclusion, we suggest that random forest would be the best fit model for this dataset considering simplicity as well as model performance.</p>
</div>